{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6df3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 18753 artículos de la base de datos.\n",
      "\n",
      "Limpiando y preprocesando el texto de los artículos...\n",
      "\n",
      "Ejemplo de limpieza:\n",
      "--- TEXTO ORIGINAL ---\n",
      "The Fight Aging! Disclaimer Please read this disclaimer carefully. It is a commonsense statement that should apply to all health information you find online. Your health is valuable and easily damaged. What is good advice for one person may not be good advice for another: people vary considerably in health matters. Information provided on Fight Aging! should always be discussed with a qualified physician. It is not intended to replace the relationship between you and your physician. It is recomm\n",
      "\n",
      "--- TEXTO LIMPIO ---\n",
      "disclaimer please read disclaimer carefully commonsense statement apply health information find online health valuable easily damaged good advice person good advice another vary considerably health matters information provided always discussed qualified physician intended replace relationship physician recommended follow topics interest internet powerful tool take advantage time cautious skeptical search support claim claim merit many reputable sources information discuss claim learn read primar\n",
      "\n",
      "DataFrame con texto limpio guardado en 'articles_cleaned.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Cela 1\n",
    "# PASO 1: PREPARACIÓN DE DATOS\n",
    "#\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def load_data_from_db(db_name=\"fightaging_articles.db\"):\n",
    "    \"\"\"Carga los artículos de la base de datos a un DataFrame de Pandas.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        df = pd.read_sql_query(\"SELECT * FROM articles\", conn)\n",
    "        conn.close()\n",
    "        df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "        print(f\"Se cargaron {len(df)} artículos de la base de datos.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Función para limpiar el texto:\n",
    "    1. Convierte a minúsculas.\n",
    "    2. Elimina puntuación y números.\n",
    "    3. Separa el texto en palabras (tokenización).\n",
    "    4. Elimina \"stop words\" (palabras comunes en inglés).\n",
    "    5. Elimina palabras muy cortas (1 o 2 letras).\n",
    "    \"\"\"\n",
    "    # 1. Minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Eliminar puntuación y números usando expresiones regulares\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenización (dividir en palabras)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Eliminar stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Añadimos algunas palabras personalizadas que son muy comunes en este contexto\n",
    "    # pero que no aportan mucho significado para diferenciar temas.\n",
    "    custom_stop_words = [\n",
    "        'welcome', 'fight', 'also', 'study', 'research', 'aging', 'age', 'may', \n",
    "        'however', 'results', 'data', 'found', 'open', 'access', \n",
    "        'great', 'deal', 'one', 'even', 'work', 'life', 'people', 'years', 'healthy',\n",
    "        'life', 'longevity', 'human'\n",
    "    ]\n",
    "    \n",
    "    stop_words.update(custom_stop_words)\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Eliminar palabras muy cortas\n",
    "    filtered_tokens = [word for word in filtered_tokens if len(word) > 2]\n",
    "    \n",
    "    # Unir las palabras de nuevo en un solo string\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "# --- Ejecución del Paso 1 ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    articles_df = load_data_from_db()\n",
    "\n",
    "    if not articles_df.empty:\n",
    "        # Combinamos título y cuerpo para un análisis más completo\n",
    "        articles_df['full_text'] = articles_df['title'] + ' ' + articles_df['body']\n",
    "        \n",
    "        print(\"\\nLimpiando y preprocesando el texto de los artículos...\")\n",
    "        # Aplicamos la función de limpieza a cada artículo\n",
    "        articles_df['cleaned_text'] = articles_df['full_text'].apply(clean_text)\n",
    "        \n",
    "        # Mostramos una comparación del antes y el después\n",
    "        print(\"\\nEjemplo de limpieza:\")\n",
    "        print(\"--- TEXTO ORIGINAL ---\")\n",
    "        print(articles_df['full_text'].iloc[1][:500]) # Muestra los primeros 500 caracteres\n",
    "        print(\"\\n--- TEXTO LIMPIO ---\")\n",
    "        print(articles_df['cleaned_text'].iloc[1][:500])\n",
    "        \n",
    "        # Guardamos el DataFrame procesado para no tener que repetir este paso\n",
    "        articles_df.to_pickle(\"articles_cleaned.pkl\")\n",
    "        print(\"\\nDataFrame con texto limpio guardado en 'articles_cleaned.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be874d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con texto limpio cargado correctamente.\n",
      "\n",
      "Convirtiendo el texto a una matriz numérica con TF-IDF...\n",
      "La matriz TF-IDF ha sido creada.\n",
      "Forma de la matriz: 18753 artículos y 22919 palabras únicas.\n",
      "\n",
      "Ejemplo de las primeras 10 palabras del vocabulario aprendido:\n",
      "['aaas' 'aak' 'aaron' 'aarp' 'aav' 'aavmediated' 'aavs' 'abandon'\n",
      " 'abandoned' 'abandoning']\n"
     ]
    }
   ],
   "source": [
    "# Celda 2\n",
    "# PASO 2: VECTORIZACIÓN\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cargamos el DataFrame que limpiamos en el paso anterior\n",
    "try:\n",
    "    articles_df = pd.read_pickle(\"articles_cleaned.pkl\")\n",
    "    print(\"DataFrame con texto limpio cargado correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: El archivo 'articles_cleaned.pkl' no fue encontrado.\")\n",
    "    print(\"Por favor, ejecuta primero el script del Paso 1 para generar este archivo.\")\n",
    "    exit()\n",
    "\n",
    "# Nos aseguramos de que no haya textos vacíos que puedan causar problemas\n",
    "articles_df.dropna(subset=['cleaned_text'], inplace=True)\n",
    "corpus = articles_df['cleaned_text']\n",
    "\n",
    "# --- Vectorización con TF-IDF ---\n",
    "# TF-IDF es una técnica para convertir la importancia de una palabra en un documento en un número\n",
    "# max_df=0.95: ignora palabras que aparecen en más del 95% de los documentos (demasiado comunes)\n",
    "# min_df=5: ignora palabras que aparecen en menos de 5 documentos (demasiado raras)\n",
    "# stop_words='english': una capa extra de limpieza\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
    "\n",
    "print(\"\\nConvirtiendo el texto a una matriz numérica con TF-IDF...\")\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# La matriz tfidf_matrix contiene la representación numérica de nuestros artículos.\n",
    "# 'feature_names' son las palabras (el vocabulario) que el vectorizador ha aprendido.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"La matriz TF-IDF ha sido creada.\")\n",
    "print(f\"Forma de la matriz: {tfidf_matrix.shape[0]} artículos y {tfidf_matrix.shape[1]} palabras únicas.\")\n",
    "print(\"\\nEjemplo de las primeras 10 palabras del vocabulario aprendido:\")\n",
    "print(feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368e400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo LDA entrenado encontrado. Cargando desde 'lda_model.joblib'...\n",
      "\n",
      "--- TÓPICOS DESCUBIERTOS POR EL MODELO LDA ---\n",
      "Tópico #0: theraputic cloning ban cisd manhattan httpwwwbiomedcentralcomnews camr mcat hwang criminalize treaty cordis criminalizing httpdbscordislucgibinsrchidadbcallernhpennewsactiondsessionrcnenrcnid httpwwwcamradvocacyorgfastactionnewsaspid suk brownback deuterium manf crap\n",
      "Tópico #1: brain alzheimers disease amyloid neurons gut blood cells microglia inflammation cognitive tau protein memory mice immune microbiome neurodegenerative inflammatory researchers\n",
      "Tópico #2: science extension medical medicine sens foundation funding future new time like stem progress good cryonics think world way live article\n",
      "Tópico #3: printing organs vessels artificial tissue engineering scaffold xenotransplantation blood bioprinting pig organ printed oocytes decellularization scaffolds pshc printer trachea nanofibers\n",
      "Tópico #4: hair progeria lamin progerin disc hgps reeve follicle transposable christopher hearing werner intervertebral wrn retrotransposons follicles ivd deafness transposons hutchinsongilford\n",
      "Tópico #5: bone naked osteoporosis mole osteoclasts osteoblasts molerats molerat nmrs osteoblast nmr osteoclast fracture resorption bmscs bones dnamage rmr killifish density\n",
      "Tópico #6: angptl pappa humanin ldlc pcsk rasgrf cetp httpwwwlongevitymemeorg glynac tis leanness mmoll irf sparc mcl drg zebra tpe hdl ldl\n",
      "Tópico #7: tooth teeth taurine enamel axolotl dental blastema salivary planarian lacrimal referendum cavities deer lizards pulp dentin acinar planarians diary scotsman\n",
      "Tópico #8: cells stem cell senescent tissue cancer immune heart regeneration bone patients researchers skin therapy regenerative blood muscle new mice adult\n",
      "Tópico #9: mitochondrial cells mice restriction dna damage calorie exercise mitochondria lifespan risk cellular species muscle effects cell function agerelated genes studies\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import joblib  # Librería para guardar y cargar modelos\n",
    "import os      # Para verificar si el archivo existe\n",
    "\n",
    "# --- Función para mostrar los tópicos (sin cambios) ---\n",
    "def display_topics(model, feature_names, n_top_words):\n",
    "    print(\"\\n--- TÓPICOS DESCUBIERTOS POR EL MODELO LDA ---\")\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Tópico #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "# --- Aplicamos el Modelo LDA (Paso 3 con lógica de guardado) ---\n",
    "\n",
    "# Definimos el nombre del archivo donde se guardará el modelo\n",
    "lda_model_file = 'lda_model.joblib'\n",
    "\n",
    "# Definimos los parámetros del modelo\n",
    "n_topics = 20\n",
    "n_top_words = 20\n",
    "\n",
    "# --- LÓGICA PARA CARGAR O ENTRENAR EL MODELO ---\n",
    "if os.path.exists(lda_model_file):\n",
    "    print(f\"✅ Modelo LDA entrenado encontrado. Cargando desde '{lda_model_file}'...\")\n",
    "    lda = joblib.load(lda_model_file)\n",
    "else:\n",
    "    # Si el modelo no existe, lo entrenamos\n",
    "    print(f\"Modelo no encontrado. Entrenando un nuevo modelo LDA para encontrar {n_topics} tópicos...\")\n",
    "    \n",
    "    # NOTA: Este bloque asume que 'tfidf_matrix' del Paso 2 está en memoria.\n",
    "    \n",
    "    # Creamos y entrenamos el modelo LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(tfidf_matrix)\n",
    "    \n",
    "    # Guardamos el modelo recién entrenado para la próxima vez\n",
    "    print(f\"✅ Modelo entrenado. Guardando en '{lda_model_file}'...\")\n",
    "    joblib.dump(lda, lda_model_file)\n",
    "\n",
    "# --- Mostramos los tópicos encontrados ---\n",
    "# Esta parte se ejecuta siempre, ya sea con el modelo cargado o el recién entrenado.\n",
    "# NOTA: Asume que 'feature_names' del Paso 2 está en memoria.\n",
    "display_topics(lda, feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
