{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 18753 articles from 'fightaging_articles.db'.\n",
      "\n",
      "‚úÖ Successfully loaded 18753 documents into LangChain.\n",
      "\n",
      "--- Example of the first document ---\n",
      "Document(metadata={'url': 'https://www.fightaging.org/archives/2004/01/welcome-to-fight-aging/', 'publish_date': '2004-01-31', 'title': 'Welcome to Fight Aging!', 'body': \"Welcome aboard! This new collaborative blog will extend the slightly bloggish daily news at the Longevity Meme into a more friendly and informative format. We will be bringing in informative, intelligent folks from the front lines in the fight against aging as authors, and plan to keep you educated and aware. As a society, we are on the verge of being able to understand, treat and ultimately prevent the degenerative conditions of aging. But we can't sit around and wait for this to happen! Join us in helping to support and document the advance of medicine for greatly extended healthy lifespans within our lifetime.\"}, page_content=\"Welcome to Fight Aging! \\n\\nWelcome aboard! This new collaborative blog will extend the slightly bloggish daily news at the Longevity Meme into a more friendly and informative format. We will be bringing in informative, intelligent folks from the front lines in the fight against aging as authors, and plan to keep you educated and aware. As a society, we are on the verge of being able to understand, treat and ultimately prevent the degenerative conditions of aging. But we can't sit around and wait for this to happen! Join us in helping to support and document the advance of medicine for greatly extended healthy lifespans within our lifetime.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "DB_NAME = \"fightaging_articles.db\"\n",
    "\n",
    "def load_data_from_db(db_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        query = \"SELECT url, publish_date, title, body FROM articles\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        df['full_text'] = df['title'] + ' \\n\\n' + df['body']\n",
    "        print(f\"‚úÖ Successfully loaded {len(df)} articles from '{db_name}'.\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"‚ùå Could not load data from the database. Error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "articles_df = load_data_from_db(DB_NAME)\n",
    "\n",
    "if not articles_df.empty:\n",
    "     loader = DataFrameLoader(articles_df, page_content_column=\"full_text\")\n",
    "\n",
    "     documents = loader.load()\n",
    "\n",
    "     # Verification Step\n",
    "     print(f\"\\n‚úÖ Successfully loaded {len(documents)} documents into LangChain.\")\n",
    "     print(\"\\n--- Example of the first document ---\")\n",
    "     # Using repr() provides a more detailed output of the object structure\n",
    "     print(repr(documents[0]))\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå DataFrame is empty. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967a14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of documents: 18753\n",
      "\n",
      "‚úÖ Successfully split 18753 documents into 90882 smaller chunks.\n",
      "\n",
      "--- Example of the first chunk ---\n",
      "Document(metadata={'url': 'https://www.fightaging.org/archives/2004/01/welcome-to-fight-aging/', 'publish_date': '2004-01-31', 'title': 'Welcome to Fight Aging!', 'body': \"Welcome aboard! This new collaborative blog will extend the slightly bloggish daily news at the Longevity Meme into a more friendly and informative format. We will be bringing in informative, intelligent folks from the front lines in the fight against aging as authors, and plan to keep you educated and aware. As a society, we are on the verge of being able to understand, treat and ultimately prevent the degenerative conditions of aging. But we can't sit around and wait for this to happen! Join us in helping to support and document the advance of medicine for greatly extended healthy lifespans within our lifetime.\", 'start_index': 0}, page_content=\"Welcome to Fight Aging! \\n\\nWelcome aboard! This new collaborative blog will extend the slightly bloggish daily news at the Longevity Meme into a more friendly and informative format. We will be bringing in informative, intelligent folks from the front lines in the fight against aging as authors, and plan to keep you educated and aware. As a society, we are on the verge of being able to understand, treat and ultimately prevent the degenerative conditions of aging. But we can't sit around and wait for this to happen! Join us in helping to support and document the advance of medicine for greatly extended healthy lifespans within our lifetime.\")\n",
      "\n",
      "--- Example of the second chunk from the same original document ---\n",
      "Document(metadata={'url': 'https://www.fightaging.org/archives/2004/02/the-fight-aging-disclaimer/', 'publish_date': '2004-02-01', 'title': 'The Fight Aging! Disclaimer', 'body': 'Please read this disclaimer carefully. It is a commonsense statement that should apply to all health information you find online. Your health is valuable and easily damaged. What is good advice for one person may not be good advice for another: people vary considerably in health matters. Information provided on Fight Aging! should always be discussed with a qualified physician. It is not intended to replace the relationship between you and your physician. It is recommended that you follow up with your own research on topics that interest you. The Internet is powerful tool for research. Take advantage of it! At the same time, be cautious. Be skeptical and search out support for any claim. If a claim has any merit, there will be many reputable sources of information that discuss that claim. Learn how to read primary scientific sources as a layperson, and how to research presently available treatments . Take no chances when it comes to your health! Be an informed consumer, show an interest in health matters, and take the time to learn. You will benefit in the long run. Legalese The purpose of this World Wide Web site is to compile, condense, and relay information to our visitors, as well as to provide a forum to allow others to express their views, research, and findings. While a reasonable effort is made to periodically review each document and source, Fight Aging! and the Fight Aging! authors cannot and do not warrant the accuracy, completeness, timeliness, correctness, or fitness for a particular purpose of the information or views made available through this media, or the material contained within. Neither Fight Aging! nor Fight Aging! authors shall be liable to you for any injury caused in whole or in part by any information obtained through Fight Aging! You agree by your decision to access this information that under no circumstances will Fight Aging! or Fight Aging! authors be liable to you for any decision made or action taken by you in reliance on such information or views. You should, as with any medical decision, consult with your physician prior to taking any medication or adopting lifestyle changes that could affect your health. Last Updated: May 10th 2014', 'start_index': 0}, page_content='The Fight Aging! Disclaimer')\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "if 'documents' in locals() and documents:\n",
    "    print(f\"Original number of documents: {len(documents)}\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    docs_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully split {len(documents)} documents into {len(docs_chunks)} smaller chunks.\")\n",
    "    \n",
    "    print(\"\\n--- Example of the first chunk ---\")\n",
    "    print(repr(docs_chunks[0]))\n",
    "\n",
    "    print(\"\\n--- Example of the second chunk from the same original document ---\")\n",
    "    print(repr(docs_chunks[1]))\n",
    "else:\n",
    "    print(\"‚ùå 'documents' list not found. Please run Cell 1 first to load the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819acd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/flan/Bodega4T1/CodeProjects/AgingLiterature/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store in 'chroma_db'...\n",
      "This will take a while, but you only have to do it once.\n",
      "\n",
      "‚úÖ Vector store created and persisted to disk.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3B: Create Embeddings and the Vector Store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Check if the chunks exist from the previous cell\n",
    "if 'docs_chunks' in locals() and docs_chunks:\n",
    "    \n",
    "    # 1. Define the path for the persistent vector store directory\n",
    "    persist_directory = 'chroma_db'\n",
    "    \n",
    "    # 2. Define the embedding model we want to use\n",
    "    # We'll use the same powerful and efficient model from your BERTopic analysis\n",
    "    embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    \n",
    "    # 3. Create and persist the vector store\n",
    "    # This is the most computationally expensive step. It will iterate through all chunks,\n",
    "    # create an embedding for each one, and store it in the Chroma database.\n",
    "    # We add a check to only run this if the database doesn't already exist.\n",
    "    if not os.path.exists(persist_directory):\n",
    "        print(f\"Creating new vector store in '{persist_directory}'...\")\n",
    "        print(\"This will take a while, but you only have to do it once.\")\n",
    "        \n",
    "        # This single command does all the work: embedding and storing\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs_chunks, \n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Vector store created and persisted to disk.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Vector store already exists in '{persist_directory}'. Loading is not needed in this step.\")\n",
    "        # If it already exists, we can simply load it in the next cell like this:\n",
    "        # vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå 'docs_chunks' not found. Please run Cell 2 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4B: Build and Run the RAG Chain (DeepSeek Version)\n",
    "import getpass\n",
    "import os\n",
    "from langchain_deepseek.chat_models import ChatDeepSeek  # Changed: Import ChatDeepSeek\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK\")\n",
    "\n",
    "# --- 1. SET UP API KEY ---\n",
    "# Changed: Ask for the DeepSeek API key\n",
    "if 'DEEPSEEK_API_KEY' not in os.environ:\n",
    "    os.environ['DEEPSEEK_API_KEY'] = getpass.getpass('Enter your DeepSeek API Key: ')\n",
    "\n",
    "# --- 2. LOAD THE COMPONENTS ---\n",
    "\n",
    "# The vector store and retriever part remains exactly the same\n",
    "persist_directory = 'chroma_db'\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory, \n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Changed: Initialize ChatDeepSeek instead of ChatGoogleGenerativeAI\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "\n",
    "# --- 3. DEFINE THE PROMPT TEMPLATE (No changes here) ---\n",
    "template = \"\"\"\n",
    "You are an expert assistant for answering questions about longevity and anti-aging research.\n",
    "Use only the following context from the FightAging.org blog to answer the question.\n",
    "If you don't know the answer from the context provided, just say that you don't know.\n",
    "Keep the answer concise and based on the provided sources.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# --- DEBUGGING FUNCTIONS ---\n",
    "def log_retrieved_documents(input_dict):\n",
    "    \"\"\"A simple function to print the retrieved documents.\"\"\"\n",
    "    print(\"--- üíª Documents Retrieved by the Retriever ---\")\n",
    "    for i, doc in enumerate(input_dict['context']):\n",
    "        print(f\"Doc {i+1} | Source: {doc.metadata.get('url', 'N/A')}\")\n",
    "        print(f\"Content Snippet: {doc.page_content[:250]}...\\n\")\n",
    "    return input_dict # Important: Pass the input through unchanged\n",
    "\n",
    "def log_final_prompt(prompt_value):\n",
    "    \"\"\"A simple function to print the final prompt sent to the LLM.\"\"\"\n",
    "    print(\"\\n--- ‚û°Ô∏è  Final Prompt Being Sent to the LLM ---\")\n",
    "    print(prompt_value.to_string())\n",
    "    print(\"---------------------------------------------\")\n",
    "    return prompt_value # Important: Pass the input through unchanged\n",
    "\n",
    "\n",
    "# --- 4. BUILD THE RAG CHAIN (Now with debugging steps) ---\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(log_retrieved_documents)  # ADDED: Peek at the retrieved docs\n",
    "    | prompt\n",
    "    | RunnableLambda(log_final_prompt)        # ADDED: Peek at the final prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- 5. INVOKE THE CHAIN AND ASK A QUESTION ---\n",
    "\n",
    "question = \"What is the blog's general view on the role of senescent cells in aging?\"\n",
    "print(f\"Asking the RAG chain: '{question}'\")\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(\"\\n--- ‚¨ÖÔ∏è  Final Answer ---\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
