{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbafa26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.py\n",
    "# Make sure to run: pip install requests beautifulsoup4 python-dateutil tqdm\n",
    "\n",
    "import requests\n",
    "import sqlite3\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- Database Functions ---\n",
    "\n",
    "# In scraper.py, replace the old database functions with these corrected versions.\n",
    "\n",
    "def setup_database(db_name=\"fightaging_articles.db\"):\n",
    "    \"\"\"Creates a database and an 'articles' table if they don't exist.\"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table with the new 'external_link' column\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        url TEXT UNIQUE,\n",
    "        publish_date TEXT,\n",
    "        title TEXT,\n",
    "        body TEXT,\n",
    "        quotes TEXT,\n",
    "        external_link TEXT \n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(f\"Database '{db_name}' is ready with the 'external_link' column.\")\n",
    "    return conn\n",
    "\n",
    "def save_to_db(conn, article_data):\n",
    "    \"\"\"Saves a single article to the database, now including the external_link.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    quotes_json = json.dumps(article_data['quotes'])\n",
    "    \n",
    "    try:\n",
    "        # Updated INSERT statement to include the 7th column\n",
    "        cursor.execute('''\n",
    "        INSERT OR IGNORE INTO articles (id, url, publish_date, title, body, quotes, external_link)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            article_data['id'],\n",
    "            article_data['url'],\n",
    "            article_data['publish_date'],\n",
    "            article_data['title'],\n",
    "            article_data['body'],\n",
    "            quotes_json,\n",
    "            article_data['external_link'] # The new data to be saved\n",
    "        ))\n",
    "        conn.commit()\n",
    "        return cursor.rowcount > 0 \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Scraping and Parsing Functions ---\n",
    "\n",
    "def parse_article(article_html):\n",
    "    \"\"\"Extracts required fields from a single article's HTML.\"\"\"\n",
    "    try:\n",
    "        # Extract the unique ID from the article tag (e.g., \"post-64033\")\n",
    "        post_id = int(article_html.get('id', 'post-0').split('-')[1])\n",
    "        if not post_id:\n",
    "            return None\n",
    "\n",
    "        # Extract Date\n",
    "        date_str = article_html.find('div', class_='post-date').get_text(strip=True)\n",
    "        # Normalize date to YYYY-MM-DD format\n",
    "        publish_date = parse(date_str).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Extract Title\n",
    "        title = article_html.find('h2', class_='title').get_text(strip=True)\n",
    "        \n",
    "        # Extract permalink URL\n",
    "        permalink_tag = article_html.find('a', id=f'permalink-{post_id}')\n",
    "        url = permalink_tag['href'] if permalink_tag else None\n",
    "        if not url:\n",
    "            return None # Skip if we can't find a URL\n",
    "        \n",
    "        # Extract Body text, clean and join paragraphs\n",
    "        post_body_div = article_html.find('div', class_='post-body')\n",
    "        body_text = post_body_div.get_text(separator=' ', strip=True) if post_body_div else \"\"\n",
    "        \n",
    "        # Extract all quotes\n",
    "        quotes = []\n",
    "        if post_body_div:\n",
    "            quote_tags = post_body_div.find_all('blockquote')\n",
    "            for quote in quote_tags:\n",
    "                quotes.append(quote.get_text(separator=' ', strip=True))\n",
    "\n",
    "        # Extract external links\n",
    "        external_link = None # Default value\n",
    "        post_body_div = article_html.find('div', class_='post-body')\n",
    "        if post_body_div:\n",
    "            # Find the span, then the 'a' tag inside it\n",
    "            newslink_span = post_body_div.find('span', class_='newslink')\n",
    "            if newslink_span:\n",
    "                link_tag = newslink_span.find('a')\n",
    "                if link_tag and 'href' in link_tag.attrs:\n",
    "                    external_link = link_tag['href']\n",
    "\n",
    "        return {\n",
    "            'id': post_id,\n",
    "            'url': url,\n",
    "            'publish_date': publish_date,\n",
    "            'title': title,\n",
    "            'body': body_text,\n",
    "            'quotes': quotes,\n",
    "            'external_link': external_link\n",
    "        }\n",
    "\n",
    "    except (AttributeError, ValueError) as e:\n",
    "        print(f\"Skipping an article due to parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_and_save_month(year, month, conn):\n",
    "    \"\"\"Scrapes all articles for a given year and month and saves them to the DB.\"\"\"\n",
    "    url = f\"https://www.fightaging.org/archives/{year}/{month:02d}/\"\n",
    "    print(f\"Scraping page: {url}\")\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Friendly Scraper for ML Project'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url,headers=headers, timeout=15)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Could not fetch page {url}. Error: {e}\")\n",
    "        return\n",
    "        \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all article containers on the page\n",
    "    articles_html = soup.find_all('article', class_='post')\n",
    "    \n",
    "    if not articles_html:\n",
    "        print(f\"No articles found for {year}-{month:02d}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(articles_html)} articles. Parsing and saving...\")\n",
    "    \n",
    "    saved_count = 0\n",
    "    for article_html in tqdm(articles_html, desc=\"Processing Articles\"):\n",
    "        article_data = parse_article(article_html)\n",
    "        if article_data:\n",
    "            if save_to_db(conn, article_data):\n",
    "                saved_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {saved_count} new articles to the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c563d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     DB_FILE = 'fightaging_articles.db'\n",
    "#     connection = setup_database(DB_FILE)\n",
    "\n",
    "#     # --- Start with a single month: September 2025 ---\n",
    "#     target_year = 2025\n",
    "#     target_month = 9\n",
    "#     scrape_and_save_month(target_year, target_month, connection)\n",
    "    \n",
    "#     connection.close()\n",
    "#     print(\"\\nScraping process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4427a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'fightaging_articles.db' is ready with the 'external_link' column.\n",
      "Starting full catalog scrape...\n",
      "Scraping page: https://www.fightaging.org/archives/2002/11/\n",
      "Found 24 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 24/24 [00:03<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 24 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2002/12/\n",
      "Found 24 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 24/24 [00:02<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 24 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/01/\n",
      "Found 41 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 41/41 [00:05<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 41 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/02/\n",
      "Found 40 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 40/40 [00:04<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 40 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/03/\n",
      "Found 47 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 47/47 [00:05<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 47 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/04/\n",
      "Found 56 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 56 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/05/\n",
      "Found 55 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 55/55 [00:07<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 55 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/06/\n",
      "Found 48 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 48/48 [00:06<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 48 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/07/\n",
      "Found 51 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 51/51 [00:06<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 51 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/08/\n",
      "Found 52 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 52/52 [00:07<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 52 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/09/\n",
      "Found 61 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 61/61 [00:09<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 61 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/10/\n",
      "Found 63 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 63/63 [00:09<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 62 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/11/\n",
      "Found 63 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 63/63 [00:08<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 63 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2003/12/\n",
      "Found 55 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 55/55 [00:07<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 55 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/01/\n",
      "Found 59 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 59/59 [00:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 59 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/02/\n",
      "Found 92 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 92/92 [00:12<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 92 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/03/\n",
      "Found 100 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 100/100 [00:13<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 98 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/04/\n",
      "Found 100 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 100/100 [00:13<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 98 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/05/\n",
      "Found 96 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 96/96 [00:12<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 96 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/06/\n",
      "Found 95 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 95/95 [00:10<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 95 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/07/\n",
      "Found 95 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 95/95 [00:10<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 95 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/08/\n",
      "Found 92 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 92/92 [00:10<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 90 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/09/\n",
      "Found 88 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 88/88 [00:09<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 87 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/10/\n",
      "Found 91 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 91/91 [00:09<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 91 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/11/\n",
      "Found 93 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 93/93 [00:10<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 93 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2004/12/\n",
      "Found 96 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|██████████| 96/96 [00:09<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 96 new articles to the database.\n",
      "Scraping page: https://www.fightaging.org/archives/2005/01/\n",
      "Found 93 articles. Parsing and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  13%|█▎        | 12/93 [00:01<00:08,  9.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m start_month = \u001b[32m11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m year == \u001b[32m2002\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_month, end_month + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mscrape_and_save_month\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# A friendly pause to avoid overwhelming the server\u001b[39;00m\n\u001b[32m     25\u001b[39m     time.sleep(\u001b[32m1\u001b[39m) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mscrape_and_save_month\u001b[39m\u001b[34m(year, month, conn)\u001b[39m\n\u001b[32m    152\u001b[39m     article_data = parse_article(article_html)\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m article_data:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msave_to_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle_data\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    155\u001b[39m             saved_count += \u001b[32m1\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m new articles to the database.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36msave_to_db\u001b[39m\u001b[34m(conn, article_data)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# Updated INSERT statement to include the 7th column\u001b[39;00m\n\u001b[32m     45\u001b[39m     cursor.execute(\u001b[33m'''\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m    INSERT OR IGNORE INTO articles (id, url, publish_date, title, body, quotes, external_link)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m    VALUES (?, ?, ?, ?, ?, ?, ?)\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m         article_data[\u001b[33m'\u001b[39m\u001b[33mexternal_link\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;66;03m# The new data to be saved\u001b[39;00m\n\u001b[32m     56\u001b[39m     ))\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     conn.commit()\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cursor.rowcount > \u001b[32m0\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sqlite3.Error \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Download all\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DB_FILE = 'fightaging_articles.db'\n",
    "    connection = setup_database(DB_FILE)\n",
    "\n",
    "    # --- Start with a single month: September 2025 ---\n",
    "    # target_year = 2025\n",
    "    # target_month = 9\n",
    "    # scrape_and_save_month(target_year, target_month, connection)\n",
    "\n",
    "    # --- To scrape the entire catalog (Nov 2002 to Sep 2025) ---\n",
    "    print(\"Starting full catalog scrape...\")\n",
    "    for year in range(2002, 2026):\n",
    "        # The archive ends in September 2025\n",
    "        end_month = 9 if year == 2025 else 12 \n",
    "        # The archive starts in November 2002\n",
    "        start_month = 11 if year == 2002 else 1\n",
    "\n",
    "        for month in range(start_month, end_month + 1):\n",
    "            scrape_and_save_month(year, month, connection)\n",
    "            # A friendly pause to avoid overwhelming the server\n",
    "            time.sleep(1) \n",
    "    \n",
    "    connection.close()\n",
    "    print(\"\\nScraping process finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
